# FiFTy 学習スクリプトのハイパーパラメータ設定ファイル

type: "gru" # モデルの種類: cnn / lstm / gru

experiment:
  seed: 42
  device: "cpu" # cpu / gpu / auto が指定可能（gpu は cuda と同義）

  # ローカルの場合の出力先のルートディレクトリ指定の例（相対パスの場合はリポジトリ直下）
  # result_dir: "result"
  # Colab で Google Drive を使う例（/content/drive/MyDrive にある任意の保存先に置換）
  result_dir: "/content/drive/MyDrive/fifty-nlp/result"

  # 中断から再開する場合の設定
  # 無効化は、null を指定（文字列ではない）
  # run_dir またはチェックポイント(.pt)を指定する
  # 相対パスで指定した場合は、result_dir からの相対パスになる
  resume_from: "2025-10-08_03-29-12_b8f5063"

data: # データセットのパス周り
  splits: ["train", "val", "test"]
  # base_dir: "~/dataset/" # ローカルのディレクトリを使う場合の例
  base_dir: "/content/drive/MyDrive/fifty-nlp/dataset/" # Drive をマウントして使う場合の例

model: # モデル構造（__init__→forward でデータが流れる順番）
  # embed_dim:      埋め込み次元数:           バイト値をベクトル化する際の出力次元数
  # hidden_dim:     全結合中間層のユニット数: 畳み込み後特徴ベクトルを写像する次元
  # num_layers:     スタック層数
  # bidirectional:  双方向か否か
  # dropout:        ドロップアウト率: 過学習防止のためにノードをランダムに無効化する確率

  cnn:
    # FiFty の 512-byte Scenario #1 (All Classes 75) の最終モデル TABLE II を参考
    # E(64), C1D (128, 27) の 128, C1D (128, 27) の 27, MP(4), D(0.1), 256
    #   - Global Average Pooling（GAP）で時系列長を1に集約
    #   - 全結合層:     256ユニット → 出力クラス数(75)
    #   - 平均精度:     77.5%
    # AP(Adaptive Average Pooling) と 75クラスへの出力をする最終全結合 F(75) はコード中
    embed_dim: 64
    conv_channels: 128 # 1D畳み込み層の出力チャネル数: 特徴マップの数。表現力に影響
    kernel_size: 27 # 畳み込みカーネル幅: 受容野の幅。局所的なバイトパターン検出の範囲
    pool_size: 4 # プーリングサイズ: ダウンサンプリング率。計算量と情報保持のバランス
    dropout: 0.1
    hidden_dim: 256
    num_blocks: 1 # 畳み込みブロック数: Conv → ReLU → Pool を繰り返す回数

  lstm:
    embed_dim: 64
    hidden_dim: 256
    num_layers: 2
    bidirectional: true
    dropout: 0.3

  gru:
    embed_dim: 64
    hidden_dim: 256
    num_layers: 2
    bidirectional: true
    dropout: 0.3

training: # 学習設定（典型的なステップ順）
  # batch_size: バッチサイズ: １ステップでネットワークに入力して処理するサンプル数
  # epochs:     エポック数:   全データセットを何周学習するか。過学習注意
  # lr:         学習率:       モデルパラメータ更新時の学習の速さ。大: 発散、小: 遅収束
  # n_subset:   サブセットサイズ: 学習データのサンプル数を制限する。最大 6144000
  # eta_min:    最小学習率: 学習率スケジューラ CosineAnnealingLR の最小値
  # ---
  # マルチプロセス読込は memmap と相性が悪いので num_workers などは設定しない

  cnn:
    batch_size: 128
    epochs: 30
    lr: 0.001
    n_subset: 61440 # 1%
    eta_min: 1.0e-7

  lstm:
    batch_size: 128
    epochs: 30
    lr: 0.0005
    n_subset: 61440 # 1%
    eta_min: 1.0e-7

  gru:
    batch_size: 128
    epochs: 30
    lr: 0.0005
    n_subset: 6144000
    eta_min: 1.0e-7
